{"cells":[{"cell_type":"markdown","metadata":{"id":"YNzBsYHukEf-"},"source":["# CNNs pentru clasificarea textelor\n","\n","Folosirea CNN-urilor pentru a clasifica textul a fost prima dată prezentată în: [Convolutional Neural Networks for Sentence Classification](https://aclanthology.org/D14-1181.pdf).\n","\n","### Arhitectura CNN\n","\n","<img src=\"https://richliao.github.io/images/YoonKim_ConvtextClassifier.png\">\n","\n","Dându-se ca input un text de $n$ cuvinte $w_{1}$, $w_{2}$, ..., $w_{n}$, transformăm fiecare cuvânt într-un vector de dimensiune $d$, rezultând vectorii $w_{1}$, $w_{2}$, ..., $w_{n}$ aparținând $R^d$. Matricea rezultată de dimensiune $d$×$n$ este apoi folosită ca input pentru un layer convoluțional care trece un *sliding window* peste text.\n","\n","Pentru fiecare window de lungime $l$:\n","\n","$u_{i}$ = [$w_{i}$, ..., $w_{i+l-1}$] $∈ R^{d×l}$, 0≤$i$≤$n-l$ \n","\n","Pentru fiecare filtru $f_{j} ∈ R^{d×l}$ calculăm <$u_{i}$, $f_{j}$> și obținem matricea $ F ∈ R^{m×n}$ (dacă am făcut padding înainte de aplicarea filtrului, astfel încât să păstrăm dimensiunea $n$ a cuvintelor), unde $m$ este numărul de filtre. Aplicăm max-pooling pe matricea $F$ rezultată, apoi aplicăm funcția de activare. În final, avem un layer *fully connected* care produce distribuția pe clase, din care rezultă clasa cu cea mai mare probabilitate."]},{"cell_type":"markdown","metadata":{"id":"Rn2QNjUCg6z0"},"source":["### Convoluții și filtre\n","\n","<img src=\"https://debajyotidatta.github.io/assets/images/conv.001.png\" width=\"300\">\n","\n","**Inputul** este format dintr-o matrice de dimensiune $n$×$d$, unde $n$ este numărul de cuvinte sau caractere, iar $d$ este lungimea reprezentării vectoriale sau lungimea vocabularului.  \n","\n","De exemplu, pentru reprezentarea vectorială a unui text la nivel de caracter, pentru $d$ = 70, numărul de caractere unice în vocabular, pe fiecare linie a matricei avem reprezentarea one-hot a unui caracter.\n","\n","<img src=\"https://debajyotidatta.github.io/assets/images/conv.002.png\" width=\"500\">\n","\n","**Filtrele** (kernels) pot avea orice lungime. Lungimea este dată de numărul de linii din filtru. Lățimea filtrului trebuie să fie aceeași cu numărul de coloane din reprezentarea vectorială ($d$).\n","\n","<img src=\"https://debajyotidatta.github.io/assets/images/conv.003.png\" width=\"300\">\n","\n","Operația de convoluție presupune multiplicarea elementelor din input și filtru, rezultând o valoare care reprezintă suma rezultatelor multiplicării. În consecință, operația de convoluție multiplică *weight*-urile din filtru cu reprezentarea vectorială a cuvintelor.\n","\n","<img src=\"https://debajyotidatta.github.io/assets/images/conv.004.png\" width=\"300\">\n","\n","<img src=\"https://debajyotidatta.github.io/assets/images/conv.005.png\" width=\"300\">\n","\n","<img src=\"https://debajyotidatta.github.io/assets/images/conv.006.png\" width=\"300\">\n","\n","Filtrul este aplicat secvențial peste input, dar, la fel ca în cazul imaginilor, putem folosi diferite valori pentru *stride* pentru a controla cât de mult de mișcă filtrul vertical. Utilizând *stride* cu o valoare $k$ putem aplica un filtrul din $k$ în $k$ linii. De exemplu, pentru un filtru cu stride = 2, filtrul va fi aplicat pe secvența de text din 2 în 2 linii și vom avea un output de dimensiune mai mică.\n","\n","<img src=\"https://debajyotidatta.github.io/assets/images/conv2.006.png\" width=\"450\">\n","\n","Filtre multiple vor produce output-uri multiple. \n","\n","<img src=\"https://debajyotidatta.github.io/assets/images/conv2.007.png\" width=\"450\">\n","\n","La următorul pas se realizează *max pooling* peste fiecare feature map rezultat din aplicarea filtrelor, iar apoi rezultatele sunt concatenate. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"T3_PYPN8fajE"},"source":["### Imagini vs Text\n","\n","Pentru a înțelege de ce o abordare folosind CNN-uri este potrivită pentru text, trebuie să ne gândim la textele noastre ca fiind niște imagini.\n","\n","Pentru exemplul următor vom cosidera că reprezentarea unei propoziții a fost făcută la nivel de cuvânt.\n","\n","De exemplu, pentru o propoziție cu lungimea maximă de 70 de cuvinte și lungimea embeddingului egală cu 300, putem crea o matrice cu valori numerice de forma 70x300 pentru a reprezenta această propoziție. Spre deosebire de imagini, în care elementele matricei sunt reprezentate de valori ale pixelilor, fiecare linie din reprezentarea vectorială a propoziției este, de fapt, reprezentarea unui cuvânt.\n","\n","În cazul imaginilor, filtrul de convoluție se deplasează și vertical și orizontal, dar în cazul textului, filtrul se deplasează doar vertical, convoluțiile sunt doar 1D. Un kernel de dimensiune (2, 300), care are dimensiunea filtrului egală cu 2 se uită doar la 2 cuvinte în același timp. Ne putem gândi, deci, la dimensionea filtrelor ca la o dimensiune a n-gramelor (bigrame, trigrame, etc.)."]},{"cell_type":"markdown","metadata":{"id":"4Z5b8qtPXLdB"},"source":["În acest laborator vom folosi datasetul IMDb movie reviews: https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"]},{"cell_type":"code","execution_count":70,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7357,"status":"ok","timestamp":1647392850585,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"aSKhMG6_U9Ch","outputId":"f107aac1-8bd5-4c2d-bd77-7a46b2c04fc1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.3.4)\n"]}],"source":["! pip install unidecode"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1647392850586,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"S3RBIBZPU2Qp","outputId":"e116e69b-7df3-4d09-814f-3fc6f1807c8b"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to C:\\Users\\Florentin-Giuliano\n","[nltk_data]     D\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import pandas as pd\n","from pprint import pprint\n","from sklearn.model_selection import train_test_split\n","from unidecode import unidecode\n","from collections import Counter\n","import nltk\n","from nltk import word_tokenize\n","nltk.download('punkt')\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1059,"status":"ok","timestamp":1647392851637,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"rDEgcB_E8uWL","outputId":"85a8b8ad-a943-4cdd-c823-2a159107558d"},"outputs":[{"data":{"text/plain":["('IMDB_Dataset.csv', <http.client.HTTPMessage at 0x1e231b222e0>)"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["from urllib.request import urlretrieve\n","urlretrieve('https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv', 'IMDB_Dataset.csv')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"elapsed":802,"status":"ok","timestamp":1647392852435,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"Q1cush6tEdAk","outputId":"3c6cdf01-a5fe-4c7d-9afb-fab40006d5f5"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>My family and I normally do not watch local mo...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Believe it or not, this was at one time the wo...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>After some internet surfing, I found the \"Home...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>One of the most unheralded great works of anim...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>It was the Sixties, and anyone with long hair ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9995</th>\n","      <td>The film maybe goes a little far, but if you l...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>This two-parter was excellent - the best since...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>Shaggy &amp; Scooby-Doo Get a Clue. It's like watc...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>Todd Rohal is a mad genius. \"Knuckleface Jones...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>Charlie Wilson's War, based on a true story, t...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10000 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                 review  sentiment\n","0     My family and I normally do not watch local mo...          1\n","1     Believe it or not, this was at one time the wo...          0\n","2     After some internet surfing, I found the \"Home...          0\n","3     One of the most unheralded great works of anim...          1\n","4     It was the Sixties, and anyone with long hair ...          0\n","...                                                 ...        ...\n","9995  The film maybe goes a little far, but if you l...          1\n","9996  This two-parter was excellent - the best since...          1\n","9997  Shaggy & Scooby-Doo Get a Clue. It's like watc...          0\n","9998  Todd Rohal is a mad genius. \"Knuckleface Jones...          1\n","9999  Charlie Wilson's War, based on a true story, t...          1\n","\n","[10000 rows x 2 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["data = pd.read_csv('IMDB_Dataset.csv')\n","data = data[:10000]\n","data"]},{"cell_type":"markdown","metadata":{"id":"qaZmejBvY2jK"},"source":["Impartim datasetul in train si test."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1647392852435,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"hVFj4JHVY19U","outputId":"a7d98a81-593e-415a-c363-3a9851e55ce8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dimensiunea datelor de train 8000\n","Dimensiunea datelor de test 2000\n"]}],"source":["train_df, test_df = train_test_split(data, test_size=0.20, random_state = 42)\n","\n","print('Dimensiunea datelor de train', len(train_df))\n","print('Dimensiunea datelor de test', len(test_df))"]},{"cell_type":"markdown","metadata":{"id":"4i3KQ4qbkcR3"},"source":["Așa cum am văzut în laboratoarele trecute, nu putem antrena un model direct pe datele sub formă de text, trebuie să transformam datele în reprezentări numerice vectoriale. \n","\n","Pentru asta, trebuie să parcurgem 2 pași:\n","\n","- **Tokenizare**: împărțirea textelor în subtexte mai mici. Astfel vom determina vocabularul setului nostru de date (setul de tokeni unici)\n","\n","- **Vectorizare**: reprezentarea în format numeric vectorial"]},{"cell_type":"markdown","metadata":{"id":"UfqLJKKSlGZ9"},"source":["Textul poate fi reprezentat fie ca o secvență de caractere, fie ca o secvență de cuvinte. Utilizarea reprezentării la nivel de cuvânt are o performanță mai bună și este mai folosită, pe când reprezentarea la nivel de caracter este utilă dacă textele au multe greșeli de scriere. "]},{"cell_type":"markdown","metadata":{"id":"5VNrOeWin9YU"},"source":["### Reprezentarea vectorială la nivel de caracter"]},{"cell_type":"markdown","metadata":{"id":"fmzl7MpVECgF"},"source":["\n","```\n","Texts: 'the mouse ran up the clock' and 'the mouse ran down'\n","```\n","\n","Pe langa caracterele prezente in textele noastre, adaugam si 2 tokeni speciali: UNK (unknown) si PAD.\n","\n","\n","```\n","Index assigned for every token: {0: 'UNK', 1: 'PAD', 2: 't', 3: 'm', 4: 'c', 5: 'h', 6: 'l', 7: 'w', 8: ' ', 9: 'a', 10: 'k', 11: 'e', 12: 'r', 13: 'u', 14: 'n', 15: 's', 16: 'd', 17: 'p', 18: 'o'}\n","```\n","\n","Reprezentarea vectoriala a celor doua texte folosind indexul corespunzator pentru fiecare caracter:\n","\n","```\n","'the mouse ran up the clock' = [2, 5, 11, 8, 3, 18, 13, 15, 11, 8, 12, 9, 14, 8, 13, 17, 8, 2, 5, 11, 8, 4, 6, 18, 4, 10]\n","'the mouse ran down' = [2, 5, 11, 8, 3, 18, 13, 15, 11, 8, 12, 9, 14, 8, 16, 18, 7, 14]\n","```\n","\n","Adaugam valori de padding la cel de-al doilea vector pentru a avea o lungime egala cu primul vector si obtinem:\n","\n","```\n","[2, 5, 11, 8, 3, 18, 13, 15, 11, 8, 12, 9, 14, 8, 16, 18, 7, 14, 1, 1, 1, 1, 1, 1, 1, 1]\n","```"]},{"cell_type":"markdown","metadata":{"id":"EUVIwpq-q1b-"},"source":["Transformăm lista de review-uri într-o listă de caractere pentru fiecare review."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1740,"status":"ok","timestamp":1647392854167,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"S3FfaGAeoKBW","outputId":"886715e1-108d-46f8-de6b-f26ead83e328"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reviews 8000\n","Labels 8000\n"]}],"source":["def transform_to_char(data):\n","\n","    reviews = []\n","    \n","    for review in data:\n","        review_cleaned = [char.lower() for char in review]\n","        reviews.append(review_cleaned)\n","\n","    return reviews\n","\n","train_reviews = transform_to_char(train_df.review)\n","train_labels = train_df.sentiment.tolist()\n","\n","print('Reviews', len(train_reviews))\n","print('Labels', len(train_labels))"]},{"cell_type":"markdown","metadata":{"id":"z9FozeZ1uyeL"},"source":["Calculam marimea vocabularului de caractere."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2073,"status":"ok","timestamp":1647392856234,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"rjMZZNaEojSq","outputId":"aa944ba3-6cdc-4893-868b-d1bbdee036f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["total chars: 128\n","{'½', '9', '“', '?', '\\xa0', \"'\", '%', '«', '¢', '\\x84', 'v', 'ñ', '\\x97', 'n', 's', 'ß', 'ô', 'é', '\\x96', '®', '¨', 'x', 'k', 'h', 'כ', 'c', '-', 'ü', ';', 'מ', '\\x95', '@', '0', '.', '\\x85', '=', 'á', 'q', 'ã', '’', '(', '»', 'j', 'b', 'f', 'a', 'y', '\\x91', '\\\\', 'î', 'ó', 'ý', ']', '1', '^', 'l', 'i', '}', 'í', 'g', '_', 'u', '+', '$', ',', '\\t', 'z', ':', 'à', '>', 't', '5', 'א', 'ð', '~', 'ן', 'w', '³', 'ä', '7', '´', '&', 'æ', 'ç', '[', '!', 'י', 'ל', '8', '/', 'ו', '…', 'ר', '‘', '–', '3', 'd', 'ë', ')', 'm', '£', 'ï', 'ג', '<', 'o', 'r', 'â', '\"', '`', '2', 'å', '|', 'ö', 'õ', 'e', 'è', '6', '*', '”', '{', 'ê', 'ø', ' ', '4', '#', 'ú', 'p', '\\x80'}\n"]}],"source":["def get_vocab(data):\n","\n","    units = set([unit for review in data for unit in review])\n","    \n","    return units\n","\n","vocab = get_vocab(train_reviews)\n","\n","print('total chars:', len(vocab))\n","print(vocab)"]},{"cell_type":"markdown","metadata":{"id":"yFpdaZWW3JSj"},"source":["Putem vedea ca avem foarte multe caractere cu accente, diferite de caracterele limbii engleze. Pentru a micsora lungimea vocabularului, putem transforma caracterele utf8 in cea mai apropiata forma ASCII a lor."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4649,"status":"ok","timestamp":1647392860877,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"5Mz0gE3uWNV0","outputId":"3c0dac72-5fa8-4b2a-8fae-caeaf0fb0ab0"},"outputs":[{"name":"stdout","output_type":"stream","text":["total chars: 70\n","{',', '\\t', 'z', '0', '.', ':', '=', 'q', '9', '<', 'o', '?', \"'\", 't', '>', 'r', '(', '5', '%', '~', '\"', 'v', 'j', 'w', 'b', '`', 'f', '2', 'a', 'y', '\\\\', '|', '7', 'n', 's', ']', '1', 'e', '-', '&', '[', '!', '6', '^', 'l', '/', 'i', '8', '*', '}', '{', 'x', ';', 'k', 'g', 'h', ' ', '4', '3', '#', '_', 'd', 'p', 'u', 'c', ')', 'm', '+', '@', '$'}\n"]}],"source":["reviews_to_ascii = [unidecode(review) for review in train_df.review]\n","train_reviews = transform_to_char(reviews_to_ascii)\n","vocab = get_vocab(train_reviews)\n","\n","print('total chars:', len(vocab))\n","print(vocab)"]},{"cell_type":"markdown","metadata":{"id":"745uYasFu4Eq"},"source":["Atribuim fiecarui caracter din vocabularul nostru un index. Vom atribui 0 caracterelor necunoscute, iar 1 va fi valoarea atribuita paddingului."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1647392860880,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"ntu0UPDNrtHh","outputId":"73113fca-b854-4e67-8a8a-2fdd879cce9e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dimensiunea vocabularului 72\n","{2: ',', 3: '\\t', 4: 'z', 5: '0', 6: '.', 7: ':', 8: '=', 9: 'q', 10: '9', 11: '<', 12: 'o', 13: '?', 14: \"'\", 15: 't', 16: '>', 17: 'r', 18: '(', 19: '5', 20: '%', 21: '~', 22: '\"', 23: 'v', 24: 'j', 25: 'w', 26: 'b', 27: '`', 28: 'f', 29: '2', 30: 'a', 31: 'y', 32: '\\\\', 33: '|', 34: '7', 35: 'n', 36: 's', 37: ']', 38: '1', 39: 'e', 40: '-', 41: '&', 42: '[', 43: '!', 44: '6', 45: '^', 46: 'l', 47: '/', 48: 'i', 49: '8', 50: '*', 51: '}', 52: '{', 53: 'x', 54: ';', 55: 'k', 56: 'g', 57: 'h', 58: ' ', 59: '4', 60: '3', 61: '#', 62: '_', 63: 'd', 64: 'p', 65: 'u', 66: 'c', 67: ')', 68: 'm', 69: '+', 70: '@', 71: '$', 0: 'UNK', 1: 'PAD'}\n"]}],"source":["char_indices = dict((c, i + 2) for i, c in enumerate(vocab))\n","indices_char = dict((i + 2, c) for i, c in enumerate(vocab))\n","\n","indices_char[0] = 'UNK'\n","char_indices['UNK'] = 0\n","\n","indices_char[1] = 'PAD'\n","char_indices['PAD'] = 1\n","\n","print('Dimensiunea vocabularului', len(indices_char))\n","print(indices_char)"]},{"cell_type":"markdown","metadata":{"id":"pXmYEKsUYHNf"},"source":["Acum putem transforma propozitiile din datasetul nostru intr-o reprezentare vectoriala, in care vom avea pentru fiecare caracter indicele corespunzator din vocabularul nostru."]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":5397,"status":"ok","timestamp":1647392866264,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"D7Xjjjgfsw8S"},"outputs":[],"source":["import numpy as np\n","\n","def vectorize_sentences(data, char_indices, one_hot = False):\n","    vectorized = []\n","    for sentences in data:\n","\n","        # transformam fiecare review in reprezentarea lui sub forma de indici ale caracterelor continute\n","        sentences_of_indices = [char_indices[w] if w in char_indices.keys() else char_indices['UNK'] for w in sentences]\n","\n","        # pentru fiecare indice putem face reprezentarea one-hot corespunzatoare\n","        # sau putem sa nu facem asta si sa adaugam un embedding layer in model care face această transformare\n","        if one_hot:\n","            sentences_of_indices = np.eye(len(char_indices))[sentences_of_indices]\n","\n","        vectorized.append(sentences_of_indices)\n","\n","    return vectorized\n","\n","train_reviews_vectorized = vectorize_sentences(train_reviews, char_indices)\n","# train_reviews_vectorized[0]"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1647392866265,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"BVwubTwmgIqH","outputId":"d8c2ddbb-61c7-4f85-882f-250bba6cd19d"},"outputs":[{"data":{"text/plain":["[1957, 370, 903, 3001, 1403]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["vectors_dim = [len(repr) for repr in train_reviews_vectorized]\n","vectors_dim[:5]"]},{"cell_type":"markdown","metadata":{"id":"m2z30TbuiFH8"},"source":["Putem vedea ca deoarece review-urile au numar diferit de caractere, in consecinta, si dimensiunile reprezentarilor vectoriale sunt diferite. \n","Vom aduce reprezentarile noastre la aceeasi dimensiune maxima.\n","\n","Definim o functie *pad* care:\n","\n","- primeste un set de review-uri si o lungime maxima\n","- scurteaza toate reprezentarile mai mari decat lungimea maxima\n","- adauga valoarea de padding (1 in cazul nostru) la reprezentarile mai scurte decat lungimea maxima"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1647392866268,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"R1NeqnXkXh0A"},"outputs":[],"source":["def pad(samples, max_length):\n","    \n","    return torch.tensor([\n","        sample[:max_length] + [1] * max(0, max_length - len(sample))\n","        for sample in samples\n","    ])"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1706,"status":"ok","timestamp":1647392867963,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"-xkuZPbUXJBR","outputId":"8a1e495a-0782-4e7d-8eea-ff28e325f688"},"outputs":[{"data":{"text/plain":["tensor([[48, 15, 58,  ..., 65, 36, 15],\n","        [63, 48, 63,  ...,  1,  1,  1],\n","        [15, 57, 48,  ...,  1,  1,  1],\n","        ...,\n","        [48, 58, 46,  ...,  1,  1,  1],\n","        [63, 39, 36,  ...,  1,  1,  1],\n","        [15, 57, 48,  ...,  1,  1,  1]])"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["train_reviews_vectorized = pad(train_reviews_vectorized, max_length = 1000)\n","train_reviews_vectorized"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1647392867964,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"wonME0qbHOdJ","outputId":"3b4cb0aa-be92-4718-8494-c61aa6522f3d"},"outputs":[{"data":{"text/plain":["torch.Size([8000, 1000])"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["train_reviews_vectorized.shape"]},{"cell_type":"markdown","metadata":{"id":"T4UI32abSffV"},"source":["Tranformăm și review-urile din setul de test într-o reprezentare vectorială"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1794,"status":"ok","timestamp":1647392869753,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"CKT9Fq7wSjKY","outputId":"295f9c77-fb31-4478-ede4-2f4b19ef0c15"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reviews 2000\n","Labels 2000\n"]}],"source":["test_reviews_to_ascii = [unidecode(review) for review in test_df.review]\n","test_reviews = transform_to_char(test_reviews_to_ascii)\n","test_labels = test_df.sentiment.tolist()\n","\n","print('Reviews', len(test_reviews))\n","print('Labels', len(test_labels))"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":1942,"status":"ok","timestamp":1647392871690,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"LvFzxZKYS8W0"},"outputs":[],"source":["test_reviews_vectorized = vectorize_sentences(test_reviews, char_indices)\n","test_reviews_vectorized = pad(test_reviews_vectorized, max_length = 1000)\n","# test_reviews_vectorized[0]"]},{"cell_type":"markdown","metadata":{"id":"nvD0iwNrH9Tu"},"source":["Vom incarca seturile noastre de date intr-un obiect din clasa [Dataset](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset)."]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1647392871692,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"tC2sg5M3H8mT"},"outputs":[],"source":["class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, samples, labels):\n","        self.samples = samples\n","        self.labels = labels\n","            \n","    def __getitem__(self, k):\n","        \"\"\"Returneaza al k-lea exemplu din dataset\"\"\"\n","        return self.samples[k], self.labels[k]\n","    \n","    def __len__(self):\n","        \"\"\"Returneaza dimensiunea datasetului\"\"\"\n","        return len(self.samples)"]},{"cell_type":"markdown","metadata":{"id":"jB3HhhKKIeDW"},"source":["Definim arhitectura modelului"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1647392871694,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"SHQYbpUhQZXs"},"outputs":[],"source":["class Model(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        \n","        # Definim un embedding layer cu un vocabular de dimensiune 72\n","        # și ca output un embedding de dimensiune 20\n","        # padding_idx este indexul din vocabular al paddingului (1, în cazul nostru)\n","        \n","        self.embedding = torch.nn.Embedding(72, 20, padding_idx=1)\n","\n","        # Definim o secvență de layere\n","        \n","        # Un layer Convolutional 1D cu 20 input channels, 32 output channels, dimensiune kernel = 3 și padding = 1\n","        # ReLU activation\n","        # 1D Maxpooling layer de dimensiune 2\n","        conv1 = torch.nn.Sequential(\n","            torch.nn.Conv1d(in_channels=20, out_channels=32, kernel_size=3, padding=1),\n","            torch.nn.ReLU(),\n","            torch.nn.MaxPool1d(kernel_size=2),\n","        )\n","        \n","        # Un layer Convolutional 1D cu 32 input channels, 32 output channels, dimensiune kernel = 5 și padding = 2\n","        # ReLU activation\n","        # 1D Maxpooling layer de dimensiune 2\n","        conv2 = torch.nn.Sequential(\n","            torch.nn.Conv1d(in_channels=32, out_channels=32, kernel_size=5, padding=2),\n","            torch.nn.ReLU(),\n","            torch.nn.MaxPool1d(kernel_size=2),\n","        )\n","        \n","        # Global Average pooling layer care, în cazul nostru, este un 1D Avgerage Pooling layer\n","        # cu dimensiunea de 250 și stride 250\n","        global_average = torch.nn.AvgPool1d(kernel_size=250, stride=250)\n","\n","        self.convolutions = torch.nn.Sequential(\n","            conv1, conv2, global_average\n","        )\n","        \n","        # Flattening layer\n","        flatten = torch.nn.Flatten()\n","        \n","        # Linear layer cu 32 input features și 2 outputs fără funcție de activare\n","        linear = torch.nn.Linear(in_features=32, out_features=2)\n","\n","        self.classifier = torch.nn.Sequential(flatten, linear)\n","        \n","    def forward(self, input):\n","        # trecem inputul prin layerul de embedding\n","        embeddings = self.embedding(input)\n","        \n","        # permutăm inputul astfel încât prima dimensiune este numărul de channels\n","        embeddings = embeddings.permute(0, 2, 1)\n","        \n","        # trecem inputul prin secvența de layere\n","        output = self.convolutions(embeddings)\n","        output = self.classifier(output)\n","        return output\n"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1647392871695,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"miX4Qat9SBTK"},"outputs":[{"ename":"AssertionError","evalue":"Torch not compiled with CUDA enabled","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32mC:\\Users\\FLOREN~1\\AppData\\Local\\Temp/ipykernel_7828/3687645909.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mDEVICE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# instanțiem modelul\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Adam optimizer cu lr = 1e-3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    905\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 907\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m     def register_backward_hook(\n","\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 578\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    599\u001b[0m             \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m                 \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    602\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    903\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[0;32m    904\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[1;32m--> 905\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    906\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    208\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_getDeviceCount'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m             raise AssertionError(\n","\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"]}],"source":["DEVICE = torch.device(\"cuda\")\n","# instanțiem modelul\n","model = Model().to(DEVICE)\n","\n","# Adam optimizer cu lr = 1e-3\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","\n","# Cross Entropy loss\n","loss_fn = torch.nn.CrossEntropyLoss()\n","\n","# training dataset and dataloader\n","# test dataset and dataloader\n","train_ds = Dataset(train_reviews_vectorized, train_labels)\n","train_dl = torch.utils.data.DataLoader(train_ds, batch_size=64, shuffle=True)\n","test_ds = Dataset(test_reviews_vectorized, test_labels)\n","test_dl = torch.utils.data.DataLoader(test_ds, batch_size=64, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"bVy9KScOVgCa"},"source":["Training loop"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15544,"status":"ok","timestamp":1647392887227,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"8jQzscISIdYG","outputId":"07a0585e-963c-48fd-86ab-25030e04cd6d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch #1\n","0.5185\n","Epoch #2\n","0.527\n","Epoch #3\n","0.623\n","Epoch #4\n","0.637\n","Epoch #5\n","0.625\n","Epoch #6\n","0.6095\n","Epoch #7\n","0.6675\n","Epoch #8\n","0.658\n","Epoch #9\n","0.657\n","Epoch #10\n","0.6855\n","Best validation accuracy 0.6855\n"]}],"source":["best_val_acc = 0\n","for epoch_n in range(10):\n","    print(f\"Epoch #{epoch_n + 1}\")\n","    model.train()\n","    for batch in train_dl:\n","        model.zero_grad()\n","\n","        inputs, targets = batch\n","        inputs = inputs.long().to(DEVICE)\n","        targets = targets.to(DEVICE)\n","\n","        output = model(inputs)\n","        loss = loss_fn(output, targets)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","    # validare\n","    model.eval()\n","    all_predictions = torch.tensor([])\n","    all_targets = torch.tensor([])\n","    for batch in test_dl:\n","        inputs, targets = batch\n","        inputs = inputs.long().to(DEVICE)\n","        targets = targets.to(DEVICE)\n","\n","        with torch.no_grad():\n","            output = model(inputs)\n","\n","        predictions = output.argmax(1)\n","        all_targets = torch.cat([all_targets, targets.detach().cpu()])\n","        all_predictions = torch.cat([all_predictions, predictions.detach().cpu()])\n","\n","    val_acc = (all_predictions == all_targets).float().mean().numpy()\n","    print(val_acc)\n","\n","    if val_acc > best_val_acc:\n","        torch.save(model.state_dict(), \"./model\")\n","        best_val_acc = val_acc\n","\n","print(\"Best validation accuracy\", best_val_acc)"]},{"cell_type":"markdown","metadata":{"id":"XdrDykrMHAwF"},"source":["### Reprezentarea vectoriala la nivel de cuvant\n"]},{"cell_type":"markdown","metadata":{"id":"CXASSJD9E3wk"},"source":["\n","```\n","Texts: 'The mouse ran up the clock' and 'The mouse ran down'\n","```\n","\n","Pe langa tokenii prezenti in textele noastre, adaugam si 2 tokeni speciali: UNK (unknown word) si PAD.\n","\n","\n","```\n","Index assigned for every token: {'UNK': 0, 'PAD': 1, 'the': 2, 'mouse': 3, 'ran': 4, 'up': 5, 'clock': 6, 'down': 7}\n","```\n","\n","Reprezentarea vectoriala a celor doua texte folosind indexul corespunzator pentru fiecare cuvant:\n","\n","```\n","'The mouse ran up the clock' = [2, 3, 4, 5, 2, 6]\n","'The mouse ran down' = [2, 3, 4, 7]\n","```\n","\n","Adaugam valori de padding la cel de-al doilea vector pentru a avea o lungime egala cu primul vector si obtinem:\n","\n","```\n","[2, 3, 4, 7, 1, 1]\n","```\n","Reprezentarea one-hot a fiecarui text:\n","\n","```\n","'The mouse ran up the clock' = [[0. 0. 1. 0. 0. 0. 0.]\n","                                [0. 0. 0. 1. 0. 0. 0.]\n","                                [0. 0. 0. 0. 1. 0. 0.]\n","                                [0. 0. 0. 0. 0. 1. 0.]\n","                                [0. 0. 1. 0. 0. 0. 0.]\n","                                [0. 0. 0. 0. 0. 0. 1.]]\n","\n","'The mouse ran down' = [[0. 0. 1. 0. 0. 0. 0. 0.]\n","                        [0. 0. 0. 1. 0. 0. 0. 0.]\n","                        [0. 0. 0. 0. 1. 0. 0. 0.]\n","                        [0. 0. 0. 0. 0. 0. 0. 1.]\n","                        [0. 1. 0. 0. 0. 0. 0. 0.]]\n","```"]},{"cell_type":"markdown","metadata":{"id":"oG8exJuIYZVh"},"source":["Impartim textele din dataset in tokeni."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1647392887230,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"aKBCVNzETzUt"},"outputs":[],"source":["def transform_to_tokens(data):\n","\n","    reviews = []\n","    for review in data:\n","        review_tokenized = word_tokenize(review.lower())\n","        reviews.append(review_tokenized)\n","\n","    return reviews"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13755,"status":"ok","timestamp":1647392900974,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"364y8q0KYGVy","outputId":"edb2e12b-414a-418a-bc31-10a5b5560beb"},"outputs":[{"name":"stdout","output_type":"stream","text":["['it', 'is', 'interesting', 'to', 'see', 'what', 'people', 'think', 'of', 'this', 'movie', ',', 'since', 'it', 'is', ',', 'in', 'fact', ',', 'quite']\n","['did', 'the', 'first', 'travesty', 'actually', 'make', 'money', '?', 'this', 'is', 'another', 'sequel', '(', 'along', 'the', 'lines', 'of', 'another', 'stakeout', ')']\n"]}],"source":["train_reviews = transform_to_tokens(train_df.review)\n","for r in train_reviews[:2]:\n","    print(r[:20])"]},{"cell_type":"markdown","metadata":{"id":"ufyvcVGLY2S8"},"source":["Construim vocabularul de tokeni"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":285,"status":"ok","timestamp":1647392901521,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"TsV0rjqiY3Ta","outputId":"dfb2ec4a-c979-4aba-d20a-1461eecdf653"},"outputs":[{"name":"stdout","output_type":"stream","text":["total words: 64266\n","['dr.tadokoro', 'jibes', 'ooooohhhh', 'advertised', 'willam', 'isolationist', 'redd', 'five-hundred-million', 'horsing', 'absurdest', 'gifts.', 'practitioner', 'tackles', 'turtlenecks', 'malta', '-screened', 'movie/theatre', 'drank', 'weaned', 'explicit', 'alluring.', 'renewal', 'cronenbergs', 'perplexity', 'sensibilities.', 'heck-of-a', 'shearer', 'received-', 'tastes', 'gurning', 'giardello', 'body-dissolving', 'dvd.', 'rewrote', 'bos', 'sharpen', 'picerni', 'context.', 'crushes', 'grand-mal', 'meet.so', 'ooooo', 'thriller.', 'barter', 'odder', 'raciest', 'amounted', 'no-talent', '3.99', 'elo', 'oppenheimer.', 'havegotten', 'trashes', 'hasidic', 'soup', 'inter', 'pappy', 'friel', 'missing.', 'reviewers', 'parole', 'c-', 'accentuated', 'doormen', 'pfieffer', 'riverbank.', 'piggy', 'brigite', 'flynn/gable', 'dilo', 'nationality', 'unfunniness', 'acting-directing', 'strong-willed', 'good/bad', 'almost-parallels', 'minnesota', 'fully-realized', 'me', 'muzaffer', 'stageplay', 'obliteration', 'disemboweled', 'guerilla-film-making', 'reining', 'other.', 'jin', 'beastly', 'ummmm', 'transatlantic', 'arthritic', 'erotically', 'love-spell', 'coherent.', 'stimulates', 'near-contemporary', 'step-father', 'videoasia', 'penned', 'ocron']\n"]}],"source":["vocab = get_vocab(train_reviews)\n","\n","print('total words:', len(vocab))\n","print(list(vocab)[:100])"]},{"cell_type":"markdown","metadata":{"id":"ROYnhY5aZk2F"},"source":["Avem un vocabular foarte mare. Vom scoate din vocabular cuvintele cu o frecventa foarte mica. \n","\n","O alta abordare pentru micscorarea vocabularului este scoaterea cuvintelor foarte frecvente (stopwords). De asemenea putem face si alti pasi de preprocesare: putem face stemming, lematizare, putem scoate punctuatia, etc.)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1647392901523,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"suBSGbXEZ21G"},"outputs":[],"source":["import operator\n","\n","def word_freq(data, min_aparitions):\n","    \n","    all_words = [words.lower() for sentences in data for words in sentences]\n","    sorted_vocab = sorted(dict(Counter(all_words)).items(), key=operator.itemgetter(1))\n","    final_vocab = [k for k,v in sorted_vocab if v > min_aparitions]\n","\n","    return final_vocab"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":604,"status":"ok","timestamp":1647392902120,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"mTvH9c6xah5I","outputId":"e7c72c3f-b971-42f0-ee24-bfd6b040ad4c"},"outputs":[{"name":"stdout","output_type":"stream","text":["['toll', 'denying', 'reported', 'shaft', 'histrionics', 'questioned', 'hurting', 'deliverance', 'backwoods', 'translate', 'dreadfully', 'bump', 'caper', 'sophomoric', 'hypnotic', 'washed', 'playful', 'stream', 'baddie', 'humiliating', 'necessity', 'incompetence', 'amidst', 'exposes', 'sirk', 'marc', 'interspersed', 'paste', 'grieving', 'tightly', 'masked', 'boil', 'marvin', 'cliffhanger', 'airing', 'video.', 'hilarious.', 'schneider', 'covert', 'holland', 'somber', 'salvage', 'sydow', 'towering', 'cannibalism', 'knives', 'shepard', 'remainder', 'two-dimensional', 'virtues', 'adapting', 'incidental', \"'it\", '..i', 'sites', 'observed', 'exploiting', 'ploy', 'czech', 'conflicted', 'stares', 'chiller', 'monument', 'raj', 'progressively', 'jox', 'tournament', 'stir', 'atop', 'shelter', 'backing', 'dives', 'dangers', 'unleashed', 'atomic', 'build-up', 'b-grade', 'garland', 'hamming', 'steele', 'orchestral', 'switched', 'opponent', 'cunning', 'chef', 'lottery', 'sumptuous', 'potent', 'spinning', 'honored', 'thankful', 'billie', 'piper', 'boards', 'hardest', 'brennan', 'distributors', 'backstory', 'sinks', 'dice']\n","9718\n"]}],"source":["vocab = word_freq(train_reviews, min_aparitions = 10)\n","\n","print(vocab[:100])\n","print(len(vocab))"]},{"cell_type":"markdown","metadata":{"id":"EhIUcWG7Yeg-"},"source":["Fiecarui token ii atribuim un indice."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1647392902121,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"ZH9CPMdib_VL"},"outputs":[],"source":["word_indices = dict((c, i + 2) for i, c in enumerate(vocab))\n","indices_word = dict((i + 2, c) for i, c in enumerate(vocab))\n","\n","indices_word[0] = 'UNK'\n","word_indices['UNK'] = 0\n","\n","indices_word[1] = 'PAD'\n","word_indices['PAD'] = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":315,"status":"ok","timestamp":1647392902429,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"bdgbyjopXh8u"},"outputs":[],"source":["# print(indices_word)"]},{"cell_type":"markdown","metadata":{"id":"csIrKV2Mc9Fh"},"source":["Acum putem transforma propozitiile din datasetul nostru intr-o reprezentare vectoriala, in care vom avea pentru fiecare cuvant indicele corespunzator din vocabular."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":458,"status":"ok","timestamp":1647392902885,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"-cPX7pE-XiCF"},"outputs":[],"source":["train_reviews_vectorized = vectorize_sentences(train_reviews, word_indices)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1647392902887,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"vg4UHdQRdZzp","outputId":"cf0381fc-6c27-45af-8139-5b83a889d687"},"outputs":[{"data":{"text/plain":["[423, 77, 186, 580, 288]"]},"execution_count":98,"metadata":{},"output_type":"execute_result"}],"source":["vectors_dim = [len(repr) for repr in train_reviews_vectorized]\n","vectors_dim[:5]"]},{"cell_type":"markdown","metadata":{"id":"kbXLorrNd5Zt"},"source":["Din nou, pentru ca textele au un numar diferit de cuvinte, trebuie sa aducem vectorii la o reprezentare de aceeași dimensiune."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":344,"status":"ok","timestamp":1647392903225,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"g-KN_NNedhzo","outputId":"8918bc04-7e55-4b5c-8bdc-2dfbdb901dae"},"outputs":[{"data":{"text/plain":["tensor([[9707, 9712, 9486,  ...,    1,    1,    1],\n","        [9635, 9719, 9619,  ...,    1,    1,    1],\n","        [9704, 9697, 9712,  ...,    1,    1,    1],\n","        ...,\n","        [9705, 9301, 9719,  ...,    1,    1,    1],\n","        [9261, 9719,  251,  ...,    1,    1,    1],\n","        [9704, 9712, 9567,  ...,    1,    1,    1]])"]},"execution_count":99,"metadata":{},"output_type":"execute_result"}],"source":["train_reviews_vectorized = pad(train_reviews_vectorized, max_length = 512)\n","train_reviews_vectorized"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1647392903226,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"nzD9dy47d38U","outputId":"d606740b-733e-47c5-9ada-456d83caa732"},"outputs":[{"data":{"text/plain":["torch.Size([8000, 512])"]},"execution_count":100,"metadata":{},"output_type":"execute_result"}],"source":["train_reviews_vectorized.shape"]},{"cell_type":"markdown","metadata":{"id":"cP5P2WPwXLSy"},"source":["# TASK\n","## Deadline: 31 martie ora 23:59.\n","\n","Formular pentru trimiterea temei: https://forms.gle/Bznaciv2MTy4kVL47\n","\n","Folosind intreg datasetul de mai sus (IMDb reviews) implementati urmatoarele cerinte:\n","1. Impartiti setul de date in 80% train, 10% validare si 10% test\n","2. Tokenizati textele si determinati vocabularul (in acest task vom lucra cu reprezentari la nivel de cuvant, NU la nivel de caracter); intrucat vocabularul poate fi foarte mare, incercati sa aplicati una dintre tehnicile mentionate in laborator (10K-20K de cuvinte ar fi o dimensiunea rezonabila a vocabularului)\n","3. Transformati textele in vectori de aceeasi dimensiune folosind indexul vocabularului (alegeti o dimensiune maxima de circa 500-1000 de tokens)\n","4. Implementati urmatoarea arhitectura:\n","    * un Embedding layer pentru vocabularul determinat, ce contine vectori de dimensiune 100\n","    * un layer dropout cu probabilitate 0.4\n","    * un layer convolutional 1D cu 100 canale de input si 128 de canale de output, dimensiunea kernelului de 3 si padding 1; asupra rezultatului aplicati un layer de [BatchNormalization](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html) cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n","    * un layer convolutional 1D cu 128 canale de input si 128 de canale de output, dimensiunea kernelului de 5 si padding 2; asupra rezultatului aplicati un layer de BatchNormalization cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n","    * un layer convolutional 1D cu 128 canale de input si 128 de canale de output, dimensiunea kernelului de 5 si padding 2; asupra rezultatului aplicati un layer de BatchNormalization cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n","    * asupra rezultatului ultimului layer, aplicati average-pooling 1D obtinand pentru fiecare canal media tuturor valorilor din vectorul sau corespunzator\n","    * un layer feed-forward (linear) cu dimensiunea inputului 128, si 2 noduri pentru output (pentru clasificare in 0/1)\n","5. Antrenati arhitectura folosind cross-entropy ca functie de loss si un optimizer la alegere. La finalul fiecarei epoci evaluati modelul pe datele de validare si salvati weighturile celui mai bun model astfel determinat\n","6. Evaluati cel mai bun model obtinut pe datele de test.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1647392903226,"user":{"displayName":"Bogdan Iordache","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64","userId":"11937921055418047811"},"user_tz":-120},"id":"uSbL6Wb7bfNi"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to C:\\Users\\Florentin-Giuliano\n","[nltk_data]     D\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"data":{"text/plain":["('IMDB_Dataset.csv', <http.client.HTTPMessage at 0x2204da351f0>)"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["# 0 Partea de definire a librariilor\n","\n","import torch\n","import pandas as pd\n","from pprint import pprint\n","from sklearn.model_selection import train_test_split\n","from unidecode import unidecode\n","from collections import Counter\n","import nltk\n","from nltk import word_tokenize\n","from urllib.request import urlretrieve\n","\n","nltk.download('punkt')\n","urlretrieve('https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv', 'IMDB_Dataset.csv')"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["40000\n"]}],"source":["#1 \n","\n","data = pd.read_csv('IMDB_Dataset.csv')\n","\n","texts = data['review']\n","labels = data['sentiment']\n","\n","x_train, x_aux, y_train, y_aux = train_test_split(texts, labels, test_size=0.20, random_state = 42)\n","x_test,x_valid, y_test, y_valid = train_test_split(x_aux,y_aux,test_size=0.50, random_state = 42)\n","\n","print(len(y_train))"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["'''\n","    ex 2\n","    Vom face tokenizarea si urmatoarele operatii, scot stop word urile, scot cuvintele cu frecventa mica, scot punctuatia, aplic si stemming\n","'''\n","import re\n","from nltk.corpus import stopwords\n","from nltk.stem.snowball import SnowballStemmer\n","\n","stemmer = SnowballStemmer('english')\n","stopwords_list = set(stopwords.words('english'))\n","\n","def processing(data):\n","    data.lower()\n","    data = re.sub('[^a-z]', ' ', data)\n","    return data\n","\n","def tokenize(data):\n","    data = word_tokenize(data)\n","    data = [i for i in data if not i in stopwords_list]\n","    return  [stemmer.stem(i) for i in data]\n","\n","vocDic = {}\n","\n","maxWords = 10000\n","\n","def processData(data,makeDic = True):\n","    newData = []\n","    n = 0\n","    for i in data:\n","        text = tokenize(processing(i))\n","        newData.append(text)\n","        if makeDic:\n","            for j in text:\n","                if not vocDic.get(j) and n < maxWords:\n","                    vocDic[j] = n\n","                    n += 1\n","                if n == maxWords:\n","                    break\n","    return newData\n","\n","x_test = processData(x_test, False)\n","x_train = processData(x_train, True)\n","x_valid = processData(x_valid,False)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# 3\n","MAX_LENGTH = 1000\n","\n","def vectorized_sentences(data): # List[List[word]]\n","    newData = []\n","    for sentence in data:\n","        vector = []\n","        for word in sentence:\n","            index = 0\n","            if vocDic.get(word):\n","                index = vocDic[word]\n","            vector.append(index)\n","        vector = vector[:MAX_LENGTH]\n","        n = MAX_LENGTH - len(vector)\n","        vector.extend(1 for i in range(n))\n","        newData.append(vector)\n","    return newData\n","\n","x_train = vectorized_sentences(x_train)\n","x_test = vectorized_sentences(x_test)\n","x_valid = vectorized_sentences(x_valid)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[2669, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 5, 14, 15, 16, 6, 17, 18, 19, 7, 20, 8, 21, 22, 23, 24, 25, 13, 26, 27, 5, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 18, 19, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 1, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 60, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"]}],"source":["# 3 -- verificare\n","\n","print(x_train[0])\n","\n","\n","# facem dataseturile\n","class MyDataset(torch.utils.data.Dataset):\n","    def __init__(self, data):\n","        super().__init__()\n","        self.data = data\n","        \n","    def __getitem__(self, k):\n","        return (\n","            self.data[\"x\"][k],\n","            self.data[\"y\"][k]\n","        )\n","    \n","    def __len__(self):\n","        return len(self.data[\"y\"])\n","\n","def generateDataloader(x, y):\n","    x = torch.tensor(x)\n","    y = torch.tensor(y.tolist())\n","    data = {\n","        'x': x,\n","        'y': y\n","    }\n","    dataset = MyDataset(data)\n","    return torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n","\n","datasetTrain = generateDataloader(x_train,y_train)\n","datasetTest = generateDataloader(x_test,y_test)\n","datasetValidate = generateDataloader(x_valid,y_valid)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# 4\n","\n","class CNN(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.emb = torch.nn.Embedding(maxWords,100,padding_idx = 1)\n","        self.model = torch.nn.Sequential(\n","            torch.nn.Dropout(0.4),\n","            torch.nn.Conv1d(in_channels  = 100,out_channels  = 128,kernel_size  = 3,padding = 1),\n","            torch.nn.BatchNorm1d(128),\n","            torch.nn.ReLU(),\n","            torch.nn.MaxPool1d(kernel_size=2),\n","            torch.nn.Conv1d(in_channels  =128,out_channels =128,kernel_size =5,padding = 2),\n","            torch.nn.BatchNorm1d(128),\n","            torch.nn.ReLU(),\n","            torch.nn.MaxPool1d(kernel_size=2),\n","            torch.nn.Conv1d(in_channels  =128,out_channels =128,kernel_size =5,padding = 2),\n","            torch.nn.BatchNorm1d(128),\n","            torch.nn.ReLU(),\n","            torch.nn.MaxPool1d(kernel_size=2),\n","            torch.nn.AvgPool1d(kernel_size=125),\n","            torch.nn.Flatten(),\n","            torch.nn.Linear(128,2)\n","        )\n","    \n","    def forward(self, data):\n","        val = self.emb(data)\n","        val = val.permute(0, 2, 1)\n","        return self.model(val)\n","\n","model = CNN()\n","\n","loss_fn = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["# 5\n","from sklearn import metrics\n","\n","minError = None\n","\n","def trainEpoch(model):\n","    model.train()\n","    print(len(datasetTrain))\n","    for x, y in datasetTrain:\n","        optimizer.zero_grad()\n","        output = model(x)\n","        loss = loss_fn(output, y.type(torch.LongTensor))\n","        loss.backward()\n","        optimizer.step()\n","\n","\n","def evalModel(model):\n","    global minError\n","    model.eval()\n","    true_labels = []\n","    predicted_labels = []\n","    mean_loss = 0\n","    with torch.no_grad():  \n","        for (x, y) in datasetValidate:\n","            output = model(x)\n","            loss = loss_fn(output, y.type(torch.LongTensor))\n","            mean_loss += loss.item()\n","            true_labels.extend(y.tolist())\n","            predicted_labels.extend(output.max(1)[1].tolist())\n","\n","    mean_loss /= len(datasetValidate)\n","\n","    if minError != None: # aici am rezolvat exercitiul 6\n","        if minError > mean_loss:\n","            minError = mean_loss\n","            torch.save(model.state_dict(), \"model.pt\")\n","    else:\n","        minError = mean_loss\n","        torch.save(model.state_dict(), \"model.pt\")\n","    acc = metrics.accuracy_score(true_labels, predicted_labels)\n","\n","    print(\"Accuracy:\", acc)\n","    print(\"Mean loss:\", mean_loss)\n","\n","\n","def trainModel(model, epochs):\n","\n","    for _ in range(epochs):\n","        trainEpoch(model)\n","        evalModel(model)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["625\n","Accuracy: 0.823\n","Mean loss: 0.419678065218503\n","625\n","Accuracy: 0.843\n","Mean loss: 0.36901676334157774\n","625\n","Accuracy: 0.8272\n","Mean loss: 0.39672634760035747\n"]}],"source":["trainModel(model,3)"]},{"cell_type":"markdown","metadata":{},"source":["Am antrenat doar in 3 epoci deoarece dureaza, 6/8 minute si ceva sa antrenezi o epoca."]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.82      0.86      0.84      2490\n","           1       0.85      0.82      0.83      2510\n","\n","    accuracy                           0.84      5000\n","   macro avg       0.84      0.84      0.84      5000\n","weighted avg       0.84      0.84      0.84      5000\n","\n"]}],"source":["# 6\n","from sklearn.metrics import classification_report\n","\n","model.load_state_dict(torch.load('model.pt'))\n","yPred = []\n","yGood = []\n","for x,y in datasetTest:\n","    y_pred = model(x)\n","    yGood.extend(y.tolist())\n","    yPred.extend(y_pred.max(1)[1].tolist())\n","\n","print(classification_report(yGood,yPred)) "]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"FINAL_lab_5.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":0}
